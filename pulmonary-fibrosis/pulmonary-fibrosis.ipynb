{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#!conda install -c conda-forge gdcm -y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!dpkg -i ../input/python3gdcm/build_1-1_amd64.deb\n!apt-get install -f","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp /usr/local/lib/gdcm.py /opt/conda/lib/python3.7/site-packages/.\n!cp /usr/local/lib/gdcmswig.py /opt/conda/lib/python3.7/site-packages/.\n!cp /usr/local/lib/_gdcmswig.so /opt/conda/lib/python3.7/site-packages/.\n!cp /usr/local/lib/libgdcm* /opt/conda/lib/python3.7/site-packages/.\n!ldconfig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# !"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\n\nimport os\nimport cv2\nimport random\nimport pickle\nimport glob\nfrom pathlib import Path\nimport gc\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport pydicom\nimport gdcm\nimport multiprocessing as mp\n\nfrom pydicom.tag import Tag\n\nimport scipy.ndimage as ndimage\nfrom scipy.ndimage import zoom\nfrom scipy.stats import kurtosis\nfrom scipy.stats import skew\n\nfrom sklearn.cluster import KMeans\n\nfrom skimage import measure, morphology, segmentation\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as Layers\nimport tensorflow.keras.models as Models\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedKFold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# !"},{"metadata":{"trusted":true},"cell_type":"code","source":"folder_path = '../input/osic-pulmonary-fibrosis-progression'\ntrain_csv = folder_path + '/train.csv'\ntest_csv = folder_path+ '/test.csv'\nsample_csv = folder_path + '/sample_submission.csv'\n\ntrain_data = pd.read_csv(train_csv)\ntest_data = pd.read_csv(test_csv)\nsample = pd.read_csv(sample_csv)\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data[\"Patient\"].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dupRows_df = train_data[train_data.duplicated(subset = ['Patient', 'Weeks'], keep = False )]\nlen(dupRows_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# !"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop_duplicates(subset=['Patient','Weeks'], keep = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.unique(train_data[['Sex', 'SmokingStatus']].values.ravel('K'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dicom_root_path = '../input/osic-pulmonary-fibrosis-progression/train/'\nPatients_id = os.listdir(dicom_root_path)\nprint(len(Patients_id))\nn_dicom_dict = {\"Patient\":[],\"n_dicom\":[],\"list_dicom\":[]}\n\nfor Patient_id in Patients_id:\n    dicom_id_path = glob.glob(dicom_root_path + Patient_id + \"/*\")\n    n_dicom_dict[\"n_dicom\"].append(len(dicom_id_path))\n    n_dicom_dict[\"Patient\"].append(Patient_id)\n    list_dicom_id = sorted([int(i.split(\"/\")[-1][:-4]) for i in dicom_id_path])\n    n_dicom_dict[\"list_dicom\"].append(list_dicom_id)\n\ndicom_pd = pd.DataFrame(n_dicom_dict)\ndicom_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"min dicom number is {min(dicom_pd['n_dicom'])}\\n\\\nmax dicom number is {max(dicom_pd['n_dicom'])}\")\n\nplt.hist(dicom_pd['n_dicom'], bins=50)\nplt.title('Number of dicom per patient');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dicom_pd['height'],dicom_pd['width'] = -1,-1\nfor Patient_id in Patients_id:\n    dicom_id_path = glob.glob(dicom_root_path + Patient_id + \"/*\")\n    for patient_dicom_id_path in dicom_id_path:\n        dicom = pydicom.dcmread(patient_dicom_id_path)\n        dicom_pd.loc[dicom_pd.Patient==Patient_id,'height'] = dicom.Rows\n        dicom_pd.loc[dicom_pd.Patient==Patient_id,'width'] = dicom.Columns\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reshape_dicom_pd = dicom_pd.loc[(dicom_pd.height!=512) | (dicom_pd.width!=512),:]\nreshape_dicom_pd = reshape_dicom_pd.reset_index(drop=True)\nreshape_dicom_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(5,2, figsize=(15, 45))\nfor idx,patient_id in enumerate(reshape_dicom_pd.head()['Patient']):\n    paths = random.sample(glob.glob(dicom_root_path + patient_id + \"/*\"),2)\n    dicom1 = pydicom.dcmread(paths[0])\n    dicom2 = pydicom.dcmread(paths[1])\n    ax[idx,0].set_title(f\"{patient_id}-{paths[0].split('/')[-1][:-4]}-{reshape_dicom_pd.loc[idx,'height']}-{reshape_dicom_pd.loc[idx,'width']}\")\n    ax[idx,0].imshow(dicom1.pixel_array, cmap=plt.cm.bone)\n    ax[idx,1].set_title(f\"patient id is {patient_id}-{paths[1].split('/')[-1][:-4]}-{reshape_dicom_pd.loc[idx,'height']}-{reshape_dicom_pd.loc[idx,'width']}\")\n    ax[idx,1].imshow(dicom2.pixel_array, cmap=plt.cm.bone)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"crop_id = reshape_dicom_pd[reshape_dicom_pd[\"height\"]!=reshape_dicom_pd[\"width\"]][\"Patient\"]\nreshape_dicom_pd['resize_type'] = 'resize'\nreshape_dicom_pd.loc[reshape_dicom_pd.Patient.isin(crop_id),'resize_type'] = 'crop'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dicom_pd['resize_type'] = 'no'\nfor idx,i in enumerate(reshape_dicom_pd['Patient']):\n    dicom_pd.loc[dicom_pd.Patient==i,'resize_type'] = reshape_dicom_pd.loc[idx,'resize_type']\ndicom_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndicom_pd=dicom_pd.merge(train_data,how='right', on=['Patient'])\nprint(dicom_pd.shape)\nprint(train_data.shape)\ndicom_pd.sample(5)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pd = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv')\ntemp_pd = pd.DataFrame(columns=train_pd.columns)\nfor i in range(len(dicom_pd)):\n    patient_pd = train_pd[train_pd.Patient==dicom_pd.iloc[i].Patient]\n    zeroweek = patient_pd['Weeks'].min()\n    if sum(patient_pd.Weeks==zeroweek)>1:\n        print(pd.unique(patient_pd.Patient))\n    temp_pd = temp_pd.append(patient_pd[patient_pd.Weeks==zeroweek].iloc[0])\ndicom_pd = pd.merge(dicom_pd, temp_pd, on=['Patient'])\ndicom_pd.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_scan(path):\n    \"\"\"\n    Loads scans from a folder and into a list.\n    \n    Parameters: path (Folder path)\n    \n    Returns: slices (List of slices)\n    \"\"\"\n    slices = [pydicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.InstanceNumber))\n    \n    try:\n        slice_thickness = np.abs(slices[0].ImagePositionPatient[2] - slices[1].ImagePositionPatient[2])\n    except:\n        try:\n            slice_thickness = np.abs(slices[0].SliceLocation - slices[1].SliceLocation)\n        except:\n            slice_thickness = slices[0].SliceThickness\n        \n    for s in slices:\n        s.SliceThickness = slice_thickness\n        \n    return slices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def transform_to_hu(slices):\n    \"\"\"\n    transform dicom.pixel_array to Hounsfield.\n    Parameters: list dicoms\n    Returns:numpy Hounsfield\n    \"\"\"\n    \n    images = np.stack([file.pixel_array for file in slices])\n    images = images.astype(np.int16)\n\n    # convert ouside pixel-values to air:\n    # I'm using <= -1000 to be sure that other defaults are captured as well\n    images[images <= -1000] = 0\n    \n    # convert to HU\n    for n in range(len(slices)):\n        \n        intercept = slices[n].RescaleIntercept\n        slope = slices[n].RescaleSlope\n        \n        if slope != 1:\n            images[n] = slope * images[n].astype(np.float64)\n            images[n] = images[n].astype(np.int16)\n            \n        images[n] += np.int16(intercept)\n    \n    return np.array(images, dtype=np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_internal_mask(image):\n    \"\"\"\n    Generates markers for a given image.\n    Parameters: image\n    Returns: Internal Marker, External Marker, Watershed Marker\n    \"\"\"\n    \n    #Creation of the internal Marker\n    marker_internal = image < -400\n    marker_internal = segmentation.clear_border(marker_internal)\n    marker_internal_labels = measure.label(marker_internal)\n    \n    areas = [r.area for r in measure.regionprops(marker_internal_labels)]\n    areas.sort()\n    \n    if len(areas) > 2:\n        for region in measure.regionprops(marker_internal_labels):\n            if region.area < areas[-2]:\n                for coordinates in region.coords:                \n                       marker_internal_labels[coordinates[0], coordinates[1]] = 0\n    \n    marker_internal = marker_internal_labels > 0\n    \n    return marker_internal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_markers(image):\n    \"\"\"\n    Generates markers for a given image.\n    Parameters: image\n    Returns: Internal Marker, External Marker, Watershed Marker\n    \"\"\"\n    \n    #Creation of the internal Marker\n    marker_internal = image < -400\n    marker_internal = segmentation.clear_border(marker_internal)\n    marker_internal_labels = measure.label(marker_internal)\n    \n    areas = [r.area for r in measure.regionprops(marker_internal_labels)]\n    areas.sort()\n    \n    if len(areas) > 2:\n        for region in measure.regionprops(marker_internal_labels):\n            if region.area < areas[-2]:\n                for coordinates in region.coords:                \n                       marker_internal_labels[coordinates[0], coordinates[1]] = 0\n    \n    marker_internal = marker_internal_labels > 0\n    \n    # Creation of the External Marker\n    external_a = ndimage.binary_dilation(marker_internal, iterations=10)\n    external_b = ndimage.binary_dilation(marker_internal, iterations=55)\n    marker_external = external_b ^ external_a\n    \n    # Creation of the Watershed Marker\n    marker_watershed = np.zeros((image.shape[0], image.shape[1]), dtype=np.int)\n    marker_watershed += marker_internal * 255\n    marker_watershed += marker_external * 128\n    \n    return marker_internal, marker_external, marker_watershed\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seperate_lungs_Watershed(image, iterations = 1):\n    \"\"\"\n    Segments lungs using various techniques.\n    \n    Parameters: image (Scan image), iterations (more iterations, more accurate mask)\n    \n    Returns: \n        - Segmented Lung\n        - Lung Filter\n        - Outline Lung\n        - Watershed Lung\n        - Sobel Gradient\n    \"\"\"\n    \n    marker_internal, marker_external, marker_watershed = generate_markers(image)\n    \n    \n    '''\n    Creation of Sobel Gradient\n    '''\n    \n    # Sobel-Gradient\n    sobel_filtered_dx = ndimage.sobel(image, 1)\n    sobel_filtered_dy = ndimage.sobel(image, 0)\n    sobel_gradient = np.hypot(sobel_filtered_dx, sobel_filtered_dy)\n    sobel_gradient *= 255.0 / np.max(sobel_gradient)\n    \n    \n    '''\n    Using the watershed algorithm\n    \n    \n    We pass the image convoluted by sobel operator and the watershed marker\n    to morphology.watershed and get a matrix matrix labeled using the \n    watershed segmentation algorithm.\n    '''\n    watershed = morphology.watershed(sobel_gradient, marker_watershed)\n    \n    \n    \n    return watershed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def crop_image(img: np.ndarray):\n    edge_pixel_value = img[0, 0]\n    mask = img != edge_pixel_value\n    return img[np.ix_(mask.any(1),mask.any(0))]\n\ndef resize_image(img: np.ndarray,reshape=(512,512)):\n    img = [cv2.resize(im,(512,512)) for im in img]\n    return img\n\ndef preprocess_img(img,local_pd):\n    #if local_pd.resize_type == 'resize':\n    #    img = [resize_image(im) for im in img]\n    if local_pd.resize_type == 'crop':\n        img = [crop_image(im) for im in img]\n    return np.array(img, dtype=np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh = [-500,0]\nf, ax = plt.subplots(4,2, figsize=(14, 14))\nfor i in range(4):\n    path = os.path.join(dicom_root_path,dicom_pd.iloc[i].Patient)\n    patient_scans = load_scan(path)\n    patient_images = transform_to_hu(patient_scans)\n    patient_images = preprocess_img(patient_images,dicom_pd.iloc[i])\n    \n    num_slices = len(patient_images)\n    patient_image = patient_images[num_slices//2]\n    \n    Mask = generate_internal_mask(patient_image)\n    \n    ax[i,0].set_title(f\"{dicom_pd.iloc[i].Patient}-{dicom_pd.iloc[i].FVC}\")\n    ax[i,0].imshow(patient_image,cmap='gray')\n    ax[i,1].imshow(Mask)\n    \nplt.show()\nplt.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh = [-500,-50]\nf, ax = plt.subplots(4,4, figsize=(28, 28))\nfor i in range(16):\n    path = os.path.join(dicom_root_path,dicom_pd.iloc[i].Patient)\n    patient_scans = load_scan(path)\n    patient_images = transform_to_hu(patient_scans)\n    patient_images = preprocess_img(patient_images,dicom_pd.iloc[i])\n    \n    num_slices = len(patient_images)\n    patient_images = patient_images[int(num_slices*0.1):int(num_slices*0.9)]\n    #patient_images = patient_images[num_slices//2]\n    patient_images_mean = np.mean(patient_images,0)\n    s_pixel = patient_images_mean.flatten()\n    s_pixel = s_pixel[np.where((s_pixel>thresh[0])&(s_pixel<thresh[1]))]\n    \n    ax[i//4,i%4].set_title(f\"{dicom_pd.iloc[i].Patient}-{dicom_pd.iloc[i].FVC}\")\n    ax[i//4,i%4].hist(s_pixel, bins=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def func_volume(patient_scan,patient_mask):\n    pixel_spacing = patient_scan.PixelSpacing\n    slice_thickness = patient_scan.SliceThickness\n    slice_volume = np.count_nonzero(patient_mask)*pixel_spacing[0]*pixel_spacing[1]*slice_thickness\n    return slice_volume\n\ndef caculate_lung_volume(patient_scans,patient_images):\n    \"\"\"\n    caculate volume of lung from mask\n    Parameters: list dicom scans,list patient CT image\n    Returns: volume cm³　(float)\n    \"\"\"\n    patient_masks = pool.map(generate_internal_mask,patient_images)\n    lung_volume = np.array(list(map(func_volume,patient_scans,patient_masks))).sum()\n\n    return lung_volume*0.001","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def caculate_histgram_statistical(patient_images,thresh = [-500,-50]):\n    \"\"\"\n    caculate hisgram kurthosis of lung hounsfield\n    Parameters: list patient CT image 512*512,thresh divide lung\n    Returns: histgram statistical characteristic(Mean,Skew,Kurthosis)\n    \"\"\"\n    statistical_characteristic = dict(Mean=0,Skew=0,Kurthosis=0)\n    num_slices = len(patient_images)\n    patient_images = patient_images[int(num_slices*0.1):int(num_slices*0.9)]\n    patient_images_mean = np.mean(patient_images,0)\n    \n    s_pixel = patient_images_mean.flatten()\n    s_pixel = s_pixel[np.where((s_pixel)>thresh[0]&(s_pixel<thresh[1]))]\n    \n    statistical_characteristic['Mean'] = np.mean(s_pixel)\n    statistical_characteristic['Skew'] = skew(s_pixel)\n    statistical_characteristic['Kurthosis'] = kurtosis(s_pixel)\n    \n    return statistical_characteristic","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lung_stat_pd = pd.DataFrame(columns=['Patient','Volume','Mean','Skew','Kurthosis'])\npool = mp.Pool()\nfor i in tqdm(range(len(dicom_pd))):\n    path = os.path.join(dicom_root_path,dicom_pd.iloc[i].Patient)\n    lung_stat_pd.loc[i,'Patient'] = dicom_pd.iloc[i].Patient\n    patient_scans = load_scan(path)\n    patient_images = transform_to_hu(patient_scans)\n    patient_images = preprocess_img(patient_images,dicom_pd.iloc[i])\n    lung_stat_pd.loc[i,'Volume'] = caculate_lung_volume(patient_scans,patient_images)\n                                    \n    patient_images = resize_image(patient_images) if dicom_pd.iloc[i].resize_type=='resize' else patient_images\n    \n    statistical_characteristic = caculate_histgram_statistical(patient_images)\n    lung_stat_pd.loc[i,'Mean'] = statistical_characteristic['Mean']\n    lung_stat_pd.loc[i,'Skew'] = statistical_characteristic['Skew']\n    lung_stat_pd.loc[i,'Kurthosis'] = statistical_characteristic['Kurthosis']\n\nlung_stat_pd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dicom_feature = pd.merge(dicom_pd, lung_stat_pd, on=['Patient'])\ndicom_feature.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dicom_feature.to_csv('CT_feature.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ! "},{"metadata":{"trusted":true},"cell_type":"code","source":"dicom_feature = pd.read_csv(\"../input/ct-featurecsv/CT_feature.csv\")\ndicom_feature.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# !"},{"metadata":{"trusted":true},"cell_type":"code","source":"dicom_feature = dicom_feature.merge(pd.read_csv(\"../input/ctfeaturecsv/CT_feature_test.csv\"),how=\"outer\")\ndicom_feature.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=train_data.merge(dicom_feature[['Patient','Volume','Mean','Skew','Kurthosis']],how=\"left\", on=['Patient'])\nprint(dicom_feature.shape)\nprint(train_data.shape)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def distribution(feature):\n    plt.figure()\n    sns.distplot(train_data[feature])\n    print(\"Max value of {} is:  {:.2f} \\nMin value of {} is:  {:.2f}\\nMean of {} is: {:.2f}\\nStandard Deviation of {} is:{:.2f}\"\\\n      .format(feature,train_data[feature].max(),feature,train_data[feature].min(),feature,train_data[feature].mean(),feature,train_data[feature].std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution(\"FVC\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution(\"Volume\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution(\"Age\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.countplot(data=train_data,x='SmokingStatus',hue='Sex');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def distribution2(feature):\n    plt.figure(figsize=(15,7))\n    plt.subplot(121)\n    for i in train_data.Sex.unique():\n        sns.distplot(train_data[train_data['Sex']==i][feature],label=i)\n    plt.title(f\"Distribution of {feature} based on Sex\")\n    plt.legend()\n\n    plt.subplot(122)\n    for i in train_data.SmokingStatus.unique():\n        sns.distplot(train_data[train_data['SmokingStatus']==i][feature],label=i)\n    plt.title(f\"Distribution of {feature}  based on Smoking Status\")\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution2(\"FVC\")\ndistribution2(\"Percent\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"females=train_data[train_data[\"Sex\"]==\"Female\"]\nmales=train_data[train_data[\"Sex\"]==\"Male\"]\nscipy.stats.ttest_ind(females[\"FVC\"],males[\"FVC\"], equal_var=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ex_smokers=train_data[train_data[\"SmokingStatus\"]==\"Ex-smoker\"]\nnever_smokers=train_data[train_data[\"SmokingStatus\"]==\"Never smoked\"]\ncurrent_smokers=train_data[train_data[\"SmokingStatus\"]==\"Currently smokes\"]\nscipy.stats.f_oneway(ex_smokers[\"Percent\"],never_smokers[\"Percent\"],current_smokers[\"Percent\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scipy.stats.ttest_ind(never_smokers[\"Percent\"],current_smokers[\"Percent\"], equal_var=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scipy.stats.ttest_ind(ex_smokers[\"Percent\"],current_smokers[\"Percent\"], equal_var=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scipy.stats.ttest_ind(never_smokers[\"Percent\"],ex_smokers[\"Percent\"], equal_var=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution2(\"Volume\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distribution2(\"Age\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.express as px\nfig = px.scatter(train_data,x=\"FVC\",y=\"Age\", facet_col=\"Sex\", color=\"SmokingStatus\",width=800, height=400)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(train_data,x=\"FVC\",y=\"Weeks\", facet_col=\"Sex\", color=\"SmokingStatus\", width=800, height=400)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = px.scatter(train_data,x=\"FVC\",y=\"Volume\", facet_col=\"Sex\", color=\"SmokingStatus\", width=800, height=400)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=pd.get_dummies(train_data,columns=[\"Sex\",\"SmokingStatus\"],drop_first=True)\ntrain_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nsns.heatmap(train_data.corr(),annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# !!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample[['Patient','Weeks']] = sample.Patient_Week.str.split(\"_\",expand = True)\nsample =  sample[['Patient','Weeks','Confidence', 'Patient_Week']]\nsample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = sample.merge(test_data.drop('Weeks', axis = 1), on = \"Patient\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Source'] = 'train'\nsample['Source'] = 'test'\n\ndata_df = train_data.append([sample])\ndata_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df=data_df.merge(dicom_feature[['Patient','Volume','Mean','Skew','Kurthosis']],how=\"left\", on=['Patient'])\nprint(dicom_feature.shape)\nprint(data_df.shape)\ndata_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df=pd.get_dummies(data_df,columns=[\"Sex\",\"SmokingStatus\"])\ndata_df.reset_index(inplace = True)\ndata_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_baseline_week(df):\n    # make a copy to not change original df    \n    _df = df.copy()\n    # ensure all Weeks values are INT and not accidentaly saved as string\n    _df['Weeks'] = _df['Weeks'].astype(int)\n    # as test data is containing all weeks, \n    _df.loc[_df.Source == 'test','min_week'] = np.nan\n    _df[\"min_week\"] = _df.groupby('Patient')['Weeks'].transform('min')\n    _df['baselined_week'] = _df['Weeks'] - _df['min_week']\n    \n    return _df \n\ndata_df = get_baseline_week(data_df)\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_baseline_FVC(df):\n    _df = df.copy()\n    base = _df.loc[_df.Weeks == _df.min_week]\n    base = base[['Patient','FVC']].copy()\n    base.columns = ['Patient','base_FVC']\n    \n    # add a row which contains the cumulated sum of rows for each patient\n    base['nb'] = 1\n    base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n    \n    # drop all except the first row for each patient (= unique rows!), containing the min_week\n    base = base[base.nb == 1]\n    base.drop('nb', axis = 1, inplace = True)\n    \n    # merge the rows containing the base_FVC on the original _df\n    _df = _df.merge(base, on = 'Patient', how = 'left')    \n    _df.drop(['min_week'], axis = 1)\n    \n    return _df\n\ndata_df = get_baseline_FVC(data_df)\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def own_MinMaxColumnScaler(df, columns):\n    \"\"\"Adds columns with scaled numeric values to range [0, 1]\n    using the formula X_scld = (X - X.min) / (X.max - X.min)\"\"\"\n    for col in columns:\n        new_col_name = col + '_scld'\n        col_min = df[col].min()\n        col_max = df[col].max()        \n        df[new_col_name] = (df[col] - col_min) / ( col_max - col_min )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df['Weeks']=data_df['Weeks'].astype(float)\ntransform_attrs = ['FVC', 'Percent', 'Age', 'Volume', 'Mean','Skew','Kurthosis','Weeks','baselined_week','base_FVC']\nown_MinMaxColumnScaler(data_df, transform_attrs)\ndata_df[data_df.Source != \"train\"].sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get back original data split\ntrain_df = data_df.loc[data_df.Source == 'train']\nsub = data_df.loc[data_df.Source == 'test']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed): \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(1998)\n\n### Features: choose which features you want to use\n# you can exclude and include features by extending this feature list\nfeatures_list = [\"base_FVC_scld\",\"Weeks_scld\",\"Age_scld\",\"Volume_scld\",\"Mean_scld\",\"Skew_scld\",\"Kurthosis_scld\",\"Sex_Male\", \"Sex_Female\",\"SmokingStatus_Ex-smoker\",\"SmokingStatus_Never smoked\",\"SmokingStatus_Currently smokes\"]\n\n### Basics for training:\nEPOCHS = 1500\nBATCH_SIZE = 256\n\n\n### LOSS; set tradeoff btw. Pinball-loss and adding score\n_lambda = 0.7 # 0.8 default\n\n\n### Optimizers\n# choose ADAM or SGD\noptimizer = 'SGD'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Learning Rate Scheduler\ndef get_lr_callback(batch_size = 64, plot = False):\n    \"\"\"Returns a lr_scheduler callback which is used for training.\n    Feel free to change the values below!\n    \"\"\"\n    LR_START   = 0.001\n    LR_MAX     = 0.0001 * BATCH_SIZE # higher batch size --> higher lr\n    LR_MIN     = 0.000001\n    # 30% of all epochs are used for ramping up the LR and then declining starts\n    LR_RAMP_EP = EPOCHS * 0.3\n    # how many epochs shall L_RMAX be sustained\n    LR_SUS_EP  = 0\n    # rate of decay\n    LR_DECAY   = 0.993\n\n    def lr_scheduler(epoch):\n            if epoch < LR_RAMP_EP:\n                lr = (LR_MAX - LR_START) / LR_RAMP_EP * epoch + LR_START\n\n            elif epoch < LR_RAMP_EP + LR_SUS_EP:\n                lr = LR_MAX\n\n            else:\n                lr = (LR_MAX - LR_MIN) * LR_DECAY ** (epoch - LR_RAMP_EP - LR_SUS_EP) + LR_MIN\n\n            return lr\n    \n    if plot == False:\n        # get the Keras-required callback with our LR for training\n        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler,verbose = False)\n        return lr_callback \n    \n    else: \n        return lr_scheduler\n    \n# plot & check the LR-Scheulder for sanity-check\nlr_scheduler_plot = get_lr_callback(batch_size = 64, plot = True)\nrng = [i for i in range(EPOCHS)]\ny = [lr_scheduler_plot(x) for x in rng]\nplt.plot(rng, y)\nprint(f\"Learning rate schedule: {y[0]:.3f} to {max(y):.3f} to {y[-1]:.3f}\")\n\n\n# logging & saving\nLOGGING = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# defining custom callbacks\nclass LogPrintingCallback(tf.keras.callbacks.Callback):\n    \n    def on_train_begin(self, logs = None):\n        #print(\"Training started\")\n        # self.val_loss = [] not used for now\n        self.val_score = []        \n        \n    def on_epoch_end(self, epoch, logs = None):\n        # self.val_loss.append(logs['val_loss']) not used for now\n        self.val_score.append(logs['val_score'])\n        if epoch % 250 == 0 or epoch == (EPOCHS -1 ):\n            print(f\"The average val-loss for epoch {epoch} is {logs['val_loss']:.2f}\"\n                  f\" and the val-score is {logs['val_score']}\")\n            \n    def on_train_end(self, lowest_val_loss, logs = None):\n        # get index of best epoch\n        best_epoch = np.argmin(self.val_score)\n        # get score in best epoch\n        best_score = self.val_score[best_epoch]\n        print(f\"Stop training, best model was found and saved in epoch {best_epoch + 1} with val-score: {best_score}.\"\n              f\" Final results in this fold (last epoch):\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_checkpont_saver_callback(fold):\n    checkpt_saver = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold,\n        monitor = 'val_score',\n        verbose = 0,\n        save_best_only = True,\n        save_weights_only = True,\n        mode = 'min',\n        save_freq = 'epoch')\n    \n    return checkpt_saver","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create constants for the loss function\nC1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n\n# define competition metric\ndef score(y_true, y_pred):\n    \"\"\"Calculate the competition metric\"\"\"\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    sigma_clip = tf.maximum(sigma, C1)\n    # Python is automatically broadcasting y_true with shape (1,0) to \n    # shape (3,0) in order to make this subtraction work\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype = tf.float32) )\n    metric = (delta / sigma_clip) * sq2 + tf.math.log(sigma_clip * sq2)\n    return K.mean(metric)\n\n# define pinball loss\ndef qloss(y_true, y_pred):\n    \"\"\"Calculate Pinball loss\"\"\"\n    # IMPORTANT: define quartiles, feel free to change here!\n    qs = [0.25, 0.50, 0.75]\n    q = tf.constant(np.array([qs]), dtype = tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q * e, (q-1) * e)\n    return K.mean(v)\n\n# combine competition metric and pinball loss to a joint loss function\ndef mloss(_lambda):\n    \"\"\"Combine Score and qloss\"\"\"\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda) * score(y_true, y_pred)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_addons as tfa\n\ndef get_model(optimizer = 'ADAM', lr = 0.01):\n    \"Creates and returns a model\"\n    # instantiate optimizer\n    optimizer = tf.keras.optimizers.Adam(lr = lr) if optimizer == 'ADAM' else tf.keras.optimizers.SGD()\n    \n    # create model    \n    inp = Layers.Input((len(features_list),), name = \"Patient\")\n    x = Layers.BatchNormalization()(inp)\n    x = tfa.layers.WeightNormalization(Layers.Dense(256, activation = \"elu\", name = \"d1\"))(x)\n    x = Layers.BatchNormalization()(x)\n    x = Layers.Dropout(0.3)(x)\n    x = tfa.layers.WeightNormalization(Layers.Dense(128, activation = \"elu\", name = \"d2\"))(x)\n    x = Layers.BatchNormalization()(x)\n    x = Layers.Dropout(0.25)(x)\n    # predicting the quantiles\n    p1 = Layers.Dense(3, activation = \"relu\", name = \"p1\")(x)\n    # quantile adjusting p1 predictions\n    p2 = Layers.Dense(3, activation = \"relu\", name = \"p2\")(x)\n    \n    # adding the cumsum to the output\n    # tf.cumsum([a, b, c]) --> [a, a + b, a + b + c]\n    preds = Layers.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis = 1), \n                     name = \"preds\")([p1, p2])\n    model = Models.Model(inp, preds, name = \"NeuralNet\")\n    model.compile(loss = mloss(_lambda), optimizer = optimizer, metrics = [score])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neuralNet = get_model(optimizer, lr = 0.01)\nneuralNet.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## GET TRAINING DATA AND TARGET VALUE\n\n# get target value\ny = train_df['FVC'].values.astype(float)\n\n\n# get training & test data\nX_train = train_df[features_list].values\nX_test = sub[features_list].values\n\n# instantiate target arrays\ntrain_preds = np.zeros((X_train.shape[0], 3))\ntest_preds = np.zeros((X_test.shape[0], 3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Non-Stratified GroupKFold-split (can be further enhanced with stratification!)\n\"\"\"K-fold variant with non-overlapping groups.\nThe same group will not appear in two different folds: in this case we dont want to have overlapping patientIDs in TRAIN and VAL-Data!\nThe folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.\"\"\"\n\nNFOLDS = 7\ngkf = GroupKFold(n_splits = NFOLDS)\n# extract Patient IDs for ensuring \ngroups = train_df['Patient'].values\n\nOOF_val_score = []\nfold = 0\n\nfor train_idx, val_idx in gkf.split(X_train, y, groups = groups):\n    fold += 1\n    print(f\"FOLD {fold}:\")\n    \n    # callbacks: logging & model saving with checkpoints each fold\n    # callbacks = [get_lr_callback(BATCH_SIZE)]  # un-comment for using LRScheduler\n    reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                                                          factor = 0.4,\n                                                          patience = 150,\n                                                          verbose = 1,\n                                                        epsilon = 1e-4,\n                                                          mode = 'min')\n    \n    callbacks = [reduce_lr_loss]\n    \n    if LOGGING == True:\n        callbacks +=  [get_checkpont_saver_callback(fold),                     \n                     LogPrintingCallback()]\n\n    # build and train model\n    model = get_model(optimizer, lr = 0.01)\n    history = model.fit(X_train[train_idx], y[train_idx], \n              batch_size = BATCH_SIZE, \n              epochs = EPOCHS, \n              validation_data = (X_train[val_idx], y[val_idx]), \n              callbacks = callbacks,\n              verbose = 0) \n    \n    # evaluate\n    print(\"Train:\", model.evaluate(X_train[train_idx], y[train_idx], verbose = 0, batch_size = BATCH_SIZE, return_dict = True))\n    print(\"Val:\", model.evaluate(X_train[val_idx], y[val_idx], verbose = 0, batch_size = BATCH_SIZE, return_dict = True))\n    \n    ## Load best model to make pred\n    model.load_weights('fold-%i.h5'%fold)\n    train_preds[val_idx] = model.predict(X_train[val_idx],\n                                         batch_size = BATCH_SIZE,\n                                         verbose = 0)\n    \n    # append OOF evaluation to calculate OFF_Score\n    OOF_val_score.append(model.evaluate(X_train[val_idx], y[val_idx], verbose = 0, batch_size = BATCH_SIZE, return_dict = True)['score'])\n    \n    # predict on test set and average the predictions over all folds\n    print(\"Predicting Test...\")\n    test_preds += model.predict(X_test, batch_size = BATCH_SIZE, verbose = 0) / NFOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fetch results from history\nscore = history.history['score']\nval_score = history.history['val_score']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)\n\n# create subplots\nplt.figure(figsize = (20,5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, score, label = 'Training Accuracy')\nplt.plot(epochs_range, val_score, label = 'Validation Accuracy')\n# limit y-values for better zoom-scale. Remember that roughly -4.5 is the best possible score\n# plt.ylim(0.8 * np.mean(val_score), 1.2 * np.mean(val_score))\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label = 'Training Loss')\nplt.plot(epochs_range, val_loss, label = 'Validation Loss')\n# limit y-values for beter zoom-scale\nplt.ylim(0.3 * np.mean(val_loss), 1.8 * np.mean(val_loss))\n\nplt.legend(loc = 'upper right')\nplt.title('Training and Validation Loss')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(OOF_val_score)\n#6.865206718444824 0.7","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## FIND OPTIMIZED STANDARD-DEVIATION\nsigma_opt = mean_absolute_error(y, train_preds[:,1])\nsigma_uncertain = train_preds[:,2] - train_preds[:,0]\nsigma_mean = np.mean(sigma_uncertain)\nprint(sigma_opt, sigma_mean)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## PREPARE SUBMISSION FILE WITH OUR PREDICTIONS\nsub['FVC1'] = test_preds[:, 1]\nsub['Confidence1'] = test_preds[:,2] - test_preds[:,0]\n\n# get rid of unused data and show some non-empty data\nsubmission = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\nsubmission.loc[~submission.FVC1.isnull()].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.loc[~submission.FVC1.isnull(),'FVC'] = submission.loc[~submission.FVC1.isnull(),'FVC1']\n\nif sigma_mean < 70:\n    submission['Confidence'] = sigma_opt\nelse:\n    submission.loc[~submission.FVC1.isnull(),'Confidence'] = submission.loc[~submission.FVC1.isnull(),'Confidence1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.drop_duplicates(subset=['Patient_Week'], keep = \"first\", inplace = True)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"org_test = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\n\nfor i in range(len(org_test)):\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'FVC'] = org_test.FVC[i]\n    submission.loc[submission['Patient_Week']==org_test.Patient[i]+'_'+str(org_test.Weeks[i]), 'Confidence'] = 70","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}